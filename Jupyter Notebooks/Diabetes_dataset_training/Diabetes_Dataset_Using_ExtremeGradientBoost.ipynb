{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7179e97",
   "metadata": {},
   "source": [
    "Steps\n",
    "1. Load the Required Libraries\n",
    "2. Read the data from the dataset\n",
    "3. Handling missing values and replacing missing values with nan from numpy and replace with mean of all the other values\n",
    "4. Encoding the categorical data\n",
    "5. Feature Engineering\n",
    "6. Split the attribites into dependent and independent attributes\n",
    "7. Splitting the dataset intro training set and test set\n",
    "8. Train the Extreme Gradient Boost Model\n",
    "9. Retraining the model using Resampled data\n",
    "10. Evaluate the retrained model\n",
    "11. Feature Importance Visualization\n",
    "12. Apply Shapley Additive Technique to the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa6cc254",
   "metadata": {},
   "source": [
    "1. Load the Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581de5a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Frank\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\dtale\\utils.py:18: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import parse_version\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import dtale\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import shap\n",
    "shap.initjs()\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split,KFold,cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from xgboost import XGBClassifier as eXTremeGradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report,roc_auc_score,recall_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e09849b",
   "metadata": {},
   "source": [
    "2. Read the data from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf06bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"C:\\\\Users\\\\Frank\\\\OneDrive\\\\Documentos\\\\ResearchPapers\\\\Diabetes_prediction_training\\\\Diabetes_Training\\\\Diabetes_prediction_training\\Datasets\\\\diabetes_dataset.csv\", encoding='latin1')\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(\"\\nData types:\\n\", df.dtypes)\n",
    "print(\"\\nMissing values:\\n\", df.isnull().sum())\n",
    "\n",
    "# Visualize class distribution\n",
    "plt.figure(figsize=(8,5))\n",
    "sns.countplot(x='diabetes', data=df)\n",
    "plt.title('Class Distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58dbbe58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns with explanation\n",
    "cols_to_drop = ['year','location','race:AfricanAmerican','race:Asian',\n",
    "               'race:Caucasian','race:Hispanic','race:Other']\n",
    "print(f\"Dropping columns: {cols_to_drop} as they are not relevant for prediction\")\n",
    "df.drop(columns=cols_to_drop, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd81a79",
   "metadata": {},
   "source": [
    "3. Handling missing values and replacing missing values with nan from numpy and replace with mean of all the other values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ab8186",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.isnull().sum())\n",
    "print(df.isna().sum())\n",
    "dtale.show(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b10ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check unique values in categorical columns\n",
    "print(df['smoking_history'].unique())\n",
    "print(df['gender'].unique())  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236d1f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['age'].describe())       # Check min/max age\n",
    "print(df['bmi'].min())            # Check if BMI is 0 or negative"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b2dcc05",
   "metadata": {},
   "source": [
    "4. Encoding the categorical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "303aa78e",
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_mapping = {'Female': 0, 'Male': 1, 'Other': 2}\n",
    "df['gender'] = df['gender'].map(gender_mapping)\n",
    "dtale.show(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643c4d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "smoking_mapping = {'never': 0, 'not current': 1, 'current': 2, \n",
    "                  'No Info': 3, 'ever': 4, 'former': 5}\n",
    "df['smoking_history'] = df['smoking_history'].map(smoking_mapping)\n",
    "dtale.show(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98935e38",
   "metadata": {},
   "source": [
    "5. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b81b6a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add feature correlation analysis\n",
    "plt.figure(figsize=(12,8))\n",
    "corr_matrix = df.corr()\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm')\n",
    "plt.title('Feature Correlation Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed95fcff",
   "metadata": {},
   "source": [
    "6. Split the attribites into dependent and independent attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f6d5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.iloc[:, :-1]  # DataFrame with original column names\n",
    "Y = df.iloc[:, -1]   # Series with original name\n",
    "dtale.show(X, ignore_duplicate=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a904ee",
   "metadata": {},
   "source": [
    "7. Splitting the dataset intro training set and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c94146b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42, stratify=Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bffe37a",
   "metadata": {},
   "source": [
    "8. Train the Extreme Gradient Boost Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7036d7c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_params = {\n",
    "    'n_estimators': 1000,\n",
    "    'learning_rate': 0.01,\n",
    "    'max_depth': 5,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'eval_metric': 'logloss',\n",
    "    'random_state': 42\n",
    "}\n",
    "model = eXTremeGradientBoostingClassifier(**xgb_params)\n",
    "model.fit(X_train, Y_train,\n",
    "          eval_set=[(X_test, Y_test)],\n",
    "          verbose=10)\n",
    "\n",
    "# Display confusion matrix for the model on test data\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "y_pred_train = model.predict(X_test)\n",
    "cm_train = confusion_matrix(Y_test, y_pred_train)\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.heatmap(cm_train, annot=True, fmt='d', cmap='Blues')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix (Test Data)')\n",
    "plt.show()\n",
    "\n",
    "# Display evaluation matrix (classification report)\n",
    "import pandas as pd\n",
    "report_train = classification_report(Y_test, y_pred_train, output_dict=True)\n",
    "df_report_train = pd.DataFrame(report_train).transpose()\n",
    "print(df_report_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "332030e9",
   "metadata": {},
   "source": [
    "9. Retraining the model using Resampled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a0d12d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "smote_enn = SMOTE(sampling_strategy='auto', random_state=42)\n",
    "X_train_res, Y_train_res = smote_enn.fit_resample(X_train, Y_train)\n",
    "print(pd.Series(Y_train_res).value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4e7bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = eXTremeGradientBoostingClassifier(**xgb_params)\n",
    "model2.fit(X_train_res, Y_train_res) \n",
    "Kfold = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "scores = cross_val_score(model, X_train_res, Y_train_res, cv=Kfold, scoring='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f1e96f",
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = model.feature_importances_\n",
    "features = df.columns[:-1]\n",
    "plt.barh(features, importances)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9670eb3f",
   "metadata": {},
   "source": [
    "10. Evaluate the retrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb820887",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model2.predict(X_test)\n",
    "y_proba = model2.predict_proba(X_test)[:, 1]\n",
    "sensitivity = recall_score(Y_test, y_pred, pos_label=1)\n",
    "roc_auc = roc_auc_score(Y_test, y_proba)\n",
    "cm = confusion_matrix(Y_test, y_pred)\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "specificity = tn / (tn + fp)\n",
    "\n",
    "\n",
    "report = classification_report(Y_test, y_pred, output_dict=True)\n",
    "df_report = pd.DataFrame(report).transpose()\n",
    "\n",
    "pd.set_option(\"display.precision\", 4)\n",
    "print(df_report)\n",
    "print(f\"ROC-AUC: {roc_auc:.4f}\")\n",
    "print(f\"Sensitivity: {sensitivity:.4f}\")\n",
    "print(f\"Specificity: {specificity:.4f}\")\n",
    "print(f\"Confusion Matrix:\\n{confusion_matrix(Y_test, y_pred)}\")\n",
    "print(f\"Mean Accuracy:{scores.mean():.4f} (+/- {scores.std():.4f})\")# Plot confusion matrix\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "    \n",
    "    # Plot ROC curve\n",
    "from sklearn.metrics import RocCurveDisplay\n",
    "RocCurveDisplay.from_estimator(model, X_test, Y_test)\n",
    "plt.title('ROC Curve')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26084ef9",
   "metadata": {},
   "source": [
    "11. Feature Importance Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f33770",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance analysis\n",
    "importance = model.feature_importances_\n",
    "features = df.columns[:-1]\n",
    "\n",
    "# Create DataFrame and sort\n",
    "feature_importance = pd.DataFrame({'Feature': features, 'Importance': importance})\n",
    "feature_importance = feature_importance.sort_values('Importance', ascending=False)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.barplot(x='Importance', y='Feature', data=feature_importance)\n",
    "plt.title('Feature Importance')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c98f8800",
   "metadata": {},
   "source": [
    "12. Apply Shapley Additive Technique to the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7333a9e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the SHAP values\n",
    "explainer = shap.Explainer(model2)\n",
    "shap_values = explainer(X_train_res)\n",
    "print(shap_values.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9d96f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Waterfall plot for the first observation\n",
    "shap.plots.waterfall(shap_values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c3f8b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values, X_train_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a8e8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.bar_plot(shap_values, max_display=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
