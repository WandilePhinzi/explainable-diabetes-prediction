{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa6cc254",
   "metadata": {},
   "source": [
    "1. Load the Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "581de5a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Frank\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\dtale\\utils.py:18: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import parse_version\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import dtale\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import category_encoders as ce\n",
    "from category_encoders import TargetEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer,SimpleImputer\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder,OrdinalEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split,KFold,cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report,roc_auc_score,recall_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e09849b",
   "metadata": {},
   "source": [
    "2. Read the data from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "24f30701",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"C:\\\\Users\\\\Frank\\\\OneDrive\\\\Documentos\\\\ResearchPapers\\\\Datasets\\\\diabetes_dataset.csv\", encoding='latin1')\n",
    "d=dtale.show(df)\n",
    "d.open_browser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a183cf6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()\n",
    "df.drop(columns=['weight','payer_code','medical_specialty','encounter_id','patient_nbr','admission_type_id','discharge_disposition_id','admission_source_id'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c754b1bc",
   "metadata": {},
   "source": [
    "3. Split the attribites into dependent and independent attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "59f6b81d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"475\"\n",
       "            src=\"http://WandilePhinzi:40000/dtale/iframe/2\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x19fd4c25be0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = df.iloc[:, :-1].values\n",
    "Y = df.iloc[:, -1].values\n",
    "dtale.show(Y, ignore_duplicate=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd81a79",
   "metadata": {},
   "source": [
    "3. Handling missing values and replacing missing values with nan from numpy and replace with mean of all the other values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f124175d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.replace('?', np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "720a26df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"475\"\n",
       "            src=\"http://WandilePhinzi:40000/dtale/iframe/3\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x19fd46f8cd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtale.show(df, ignore_duplicate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "286cc3f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values in each column (%):\n",
      "max_glu_serum    96420\n",
      "A1Cresult        84748\n",
      "race              2273\n",
      "diag_3            1423\n",
      "diag_2             358\n",
      "diag_1              21\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "missing_values = df.isnull().sum()\n",
    "missing_percent= df.isnull().sum() / len(df) * 100\n",
    "missing_values = missing_values[missing_values > 0].sort_values(ascending=False)\n",
    "print(\"Missing values in each column (%):\")\n",
    "print(missing_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6c09ac9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns with missing values:\n",
      "               Missing Values  Percentage (%)\n",
      "max_glu_serum         96420.0       94.746772\n",
      "A1Cresult             84748.0       83.277322\n",
      "race                   2273.0        2.233555\n",
      "diag_3                 1423.0        1.398306\n",
      "diag_2                  358.0        0.351787\n",
      "diag_1                   21.0        0.020636\n"
     ]
    }
   ],
   "source": [
    "missing_data = pd.DataFrame({\n",
    "    'Missing Values': missing_values,\n",
    "    'Percentage (%)': missing_percent\n",
    "}).sort_values(by='Missing Values', ascending=False)\n",
    "missing_data = missing_data[missing_data['Missing Values'] > 0]\n",
    "print(\"Columns with missing values:\")\n",
    "print(missing_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "97ab8186",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filled race with mode: Caucasian\n",
      "Filled gender with mode: Female\n",
      "Filled diag_1 with mode: 428\n",
      "Filled diag_2 with mode: 276\n",
      "Filled diag_3 with mode: 250\n"
     ]
    }
   ],
   "source": [
    "cat_cols_with_missing = ['race', 'gender', 'diag_1', 'diag_2', 'diag_3']\n",
    "for col in cat_cols_with_missing:\n",
    "    mode_val = df[col].mode()[0]\n",
    "    df[col] = df[col].fillna(mode_val)\n",
    "    print(f\"Filled {col} with mode: {mode_val}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4fbf17fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Frank\\AppData\\Local\\Temp\\ipykernel_17216\\18218844.py:2: FutureWarning:\n",
      "\n",
      "A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"475\"\n",
       "            src=\"http://WandilePhinzi:40000/dtale/iframe/4\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x19fd46f9d10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For race - impute with 'Unknown' category\n",
    "df['race'].fillna('Unknown', inplace=True)\n",
    "dtale.show(df, ignore_duplicate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "818e92a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Frank\\AppData\\Local\\Temp\\ipykernel_17216\\1508974081.py:4: FutureWarning:\n",
      "\n",
      "A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "\n",
      "C:\\Users\\Frank\\AppData\\Local\\Temp\\ipykernel_17216\\1508974081.py:5: FutureWarning:\n",
      "\n",
      "A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.dropna(subset=['diag_1'])\n",
    "\n",
    "# For secondary diagnoses, missing likely means no secondary diagnosis\n",
    "df['diag_2'].fillna('None', inplace=True)\n",
    "df['diag_3'].fillna('None', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "09fde60d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Frank\\AppData\\Local\\Temp\\ipykernel_17216\\3880878642.py:5: FutureWarning:\n",
      "\n",
      "A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "\n",
      "C:\\Users\\Frank\\AppData\\Local\\Temp\\ipykernel_17216\\3880878642.py:6: FutureWarning:\n",
      "\n",
      "A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df['max_glu_serum_missing'] = df['max_glu_serum'].isnull().astype(int)\n",
    "df['A1Cresult_missing'] = df['A1Cresult'].isnull().astype(int)\n",
    "\n",
    "# Then impute with 'None' (assuming missing means test wasn't performed)\n",
    "df['max_glu_serum'].fillna('None', inplace=True)\n",
    "df['A1Cresult'].fillna('None', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2c1fd7fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify numerical columns\n",
    "num_cols = df.select_dtypes(include=['int64', 'float64']).columns\n",
    "\n",
    "# Use iterative imputer for numerical variables (more sophisticated than simple mean/median)\n",
    "imputer = IterativeImputer(max_iter=10, random_state=42)\n",
    "df[num_cols] = imputer.fit_transform(df[num_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a8fc6e01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Missing values after imputation:\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# Check if any missing values remain\n",
    "print(\"\\nMissing values after imputation:\")\n",
    "print(df.isnull().sum().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2e166b80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Launching D-Tale for final verification...\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nLaunching D-Tale for final verification...\")\n",
    "d_final = dtale.show(df)\n",
    "d_final.open_browser()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b2dcc05",
   "metadata": {},
   "source": [
    "4. Encoding the categorical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "303aa78e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical columns to encode:\n",
      "['race', 'gender', 'age', 'num_procedures', 'max_glu_serum', 'A1Cresult', 'metformin', 'repaglinide', 'nateglinide', 'chlorpropamide', 'glimepiride', 'acetohexamide', 'glipizide', 'glyburide', 'tolbutamide', 'pioglitazone', 'rosiglitazone', 'acarbose', 'miglitol', 'troglitazone', 'tolazamide', 'examide', 'citoglipton', 'insulin', 'glyburide-metformin', 'glipizide-metformin', 'glimepiride-pioglitazone', 'metformin-rosiglitazone', 'metformin-pioglitazone', 'change', 'diabetesMed', 'readmitted', 'max_glu_serum_missing', 'A1Cresult_missing']\n"
     ]
    }
   ],
   "source": [
    "#Identify categorical columns (object type and low-cardinality numeric)\n",
    "categorical_cols = [col for col in df.columns \n",
    "                   if df[col].dtype == 'object' or df[col].nunique() < 10]\n",
    "\n",
    "# Exclude columns that shouldn't be encoded (like IDs or text fields)\n",
    "# For this dataset, we might exclude diagnosis codes if we want special handling\n",
    "cols_to_exclude = ['diag_1', 'diag_2', 'diag_3']  # Example - adjust as needed\n",
    "categorical_cols = [col for col in categorical_cols if col not in cols_to_exclude]\n",
    "\n",
    "print(\"Categorical columns to encode:\")\n",
    "print(categorical_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5882a180",
   "metadata": {},
   "outputs": [],
   "source": [
    "#One-hot encode categorical variables\n",
    "nominal_cols = ['race', 'gender', 'max_glu_serum', 'A1Cresult', 'change', 'diabetesMed']\n",
    "\n",
    "# Initialize OneHotEncoder\n",
    "ohe = OneHotEncoder(drop='first', sparse_output=False)  # drop first to avoid dummy trap\n",
    "\n",
    "# Fit and transform\n",
    "ohe_array = ohe.fit_transform(df[nominal_cols])\n",
    "\n",
    "# Create column names for the encoded features\n",
    "ohe_columns = ohe.get_feature_names_out(nominal_cols)\n",
    "\n",
    "# Create DataFrame from the encoded array\n",
    "ohe_df = pd.DataFrame(ohe_array, columns=ohe_columns, index=df.index)\n",
    "\n",
    "# Drop original columns and concatenate with encoded data\n",
    "df = df.drop(nominal_cols, axis=1)\n",
    "df = pd.concat([df, ohe_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c6478c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ordinal_cols = ['age']  # age groups have natural ordering\n",
    "\n",
    "# Define custom ordering for each ordinal column\n",
    "age_order = ['[0-10)', '[10-20)', '[20-30)', '[30-40)', '[40-50)', \n",
    "             '[50-60)', '[60-70)', '[70-80)', '[80-90)', '[90-100)']\n",
    "\n",
    "# Initialize OrdinalEncoder with specified categories\n",
    "ordinal_encoder = OrdinalEncoder(categories=[age_order])\n",
    "\n",
    "# Fit and transform\n",
    "df[ordinal_cols] = ordinal_encoder.fit_transform(df[ordinal_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3682600e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For diagnosis codes if we decide to encode them\n",
    "high_cardinality_cols = ['diag_1', 'diag_2', 'diag_3']  # Example\n",
    "\n",
    "# Initialize BinaryEncoder\n",
    "binary_encoder = ce.BinaryEncoder(cols=high_cardinality_cols)\n",
    "\n",
    "# Fit and transform\n",
    "df = binary_encoder.fit_transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7b30db24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_diagnosis(code):\n",
    "    if pd.isna(code) or code == 'None':\n",
    "        return 'None'\n",
    "    try:\n",
    "        code = float(code)\n",
    "        if code >= 390 and code <= 459 or code == 785:\n",
    "            return 'Circulatory'\n",
    "        elif code >= 460 and code <= 519 or code == 786:\n",
    "            return 'Respiratory'\n",
    "        elif code >= 520 and code <= 579 or code == 787:\n",
    "            return 'Digestive'\n",
    "        elif code >= 250 and code < 251:\n",
    "            return 'Diabetes'\n",
    "        else:\n",
    "            return 'Other'\n",
    "    except:\n",
    "        return 'Other'\n",
    "\n",
    "# Apply grouping\n",
    "df['diag_1_grouped'] = df['diag_1'].apply(group_diagnosis)\n",
    "df['diag_2_grouped'] = df['diag_2'].apply(group_diagnosis)\n",
    "df['diag_3_grouped'] = df['diag_3'].apply(group_diagnosis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "32c7e834",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your target column\n",
    "target_col = 'readmitted'\n",
    "\n",
    "# Select columns for target encoding - only use columns that exist\n",
    "target_encode_cols = [col for col in ['diag_1_grouped', 'diag_2_grouped', 'diag_3_grouped'] \n",
    "                    if col in df.columns]\n",
    "\n",
    "# Only proceed if we have columns to encode\n",
    "if target_encode_cols:\n",
    "    # Initialize TargetEncoder\n",
    "    target_encoder = TargetEncoder(cols=target_encode_cols)\n",
    "    \n",
    "    # Fit and transform\n",
    "    df[target_encode_cols] = target_encoder.fit_transform(df[target_encode_cols], df[target_col])\n",
    "else:\n",
    "    print(\"No valid columns found for target encoding\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b33428bb",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'diag_1'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3804\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3805\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3806\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mindex.pyx:167\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mindex.pyx:196\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7081\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7089\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mKeyError\u001b[39m: 'diag_1'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m high_cardinality_cols:\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     freq = \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m]\u001b[49m.value_counts(normalize=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m      3\u001b[39m     df[col+\u001b[33m'\u001b[39m\u001b[33m_freq\u001b[39m\u001b[33m'\u001b[39m] = df[col].map(freq)\n\u001b[32m      4\u001b[39m     df.drop(col, axis=\u001b[32m1\u001b[39m, inplace=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\pandas\\core\\frame.py:4102\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4100\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.columns.nlevels > \u001b[32m1\u001b[39m:\n\u001b[32m   4101\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._getitem_multilevel(key)\n\u001b[32m-> \u001b[39m\u001b[32m4102\u001b[39m indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4103\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[32m   4104\u001b[39m     indexer = [indexer]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3807\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   3808\u001b[39m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc.Iterable)\n\u001b[32m   3809\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[32m   3810\u001b[39m     ):\n\u001b[32m   3811\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[32m-> \u001b[39m\u001b[32m3812\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m   3813\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m   3814\u001b[39m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[32m   3815\u001b[39m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[32m   3816\u001b[39m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[32m   3817\u001b[39m     \u001b[38;5;28mself\u001b[39m._check_indexing_error(key)\n",
      "\u001b[31mKeyError\u001b[39m: 'diag_1'"
     ]
    }
   ],
   "source": [
    "for col in high_cardinality_cols:\n",
    "    freq = df[col].value_counts(normalize=True)\n",
    "    df[col+'_freq'] = df[col].map(freq)\n",
    "    df.drop(col, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a904ee",
   "metadata": {},
   "source": [
    "5. Splitting the dataset intro training set and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c94146b5",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\IPython\\core\\async_helpers.py:128\u001b[39m, in \u001b[36m_pseudo_sync_runner\u001b[39m\u001b[34m(coro)\u001b[39m\n\u001b[32m    120\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    121\u001b[39m \u001b[33;03mA runner that does not really allow async execution, and just advance the coroutine.\u001b[39;00m\n\u001b[32m    122\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    125\u001b[39m \u001b[33;03mCredit to Nathaniel Smith\u001b[39;00m\n\u001b[32m    126\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    127\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m128\u001b[39m     \u001b[43mcoro\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    129\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    130\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m exc.value\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\IPython\\core\\interactiveshell.py:3315\u001b[39m, in \u001b[36mInteractiveShell.run_cell_async\u001b[39m\u001b[34m(self, raw_cell, store_history, silent, shell_futures, transformed_cell, preprocessing_exc_tuple, cell_id)\u001b[39m\n\u001b[32m   3313\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m store_history:\n\u001b[32m   3314\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.history_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3315\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mhistory_manager\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstore_inputs\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mexecution_count\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcell\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mraw_cell\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3316\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m silent:\n\u001b[32m   3317\u001b[39m     \u001b[38;5;28mself\u001b[39m.logger.log(cell, raw_cell)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\IPython\\core\\history.py:969\u001b[39m, in \u001b[36mHistoryManager.store_inputs\u001b[39m\u001b[34m(self, line_num, source, source_raw)\u001b[39m\n\u001b[32m    966\u001b[39m \u001b[38;5;28mself\u001b[39m.input_hist_raw.append(source_raw)\n\u001b[32m    968\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.db_input_cache_lock:\n\u001b[32m--> \u001b[39m\u001b[32m969\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdb_input_cache\u001b[49m.append((line_num, source, source_raw))\n\u001b[32m    970\u001b[39m     \u001b[38;5;66;03m# Trigger to flush cache and write to DB.\u001b[39;00m\n\u001b[32m    971\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m.db_input_cache) >= \u001b[38;5;28mself\u001b[39m.db_cache_size:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\traitlets\\traitlets.py:676\u001b[39m, in \u001b[36mTraitType.__get__\u001b[39m\u001b[34m(self, obj, cls)\u001b[39m\n\u001b[32m    672\u001b[39m \u001b[38;5;129m@t\u001b[39m.overload\n\u001b[32m    673\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__get__\u001b[39m(\u001b[38;5;28mself\u001b[39m, obj: t.Any, \u001b[38;5;28mcls\u001b[39m: \u001b[38;5;28mtype\u001b[39m[t.Any]) -> G:\n\u001b[32m    674\u001b[39m     ...\n\u001b[32m--> \u001b[39m\u001b[32m676\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__get__\u001b[39m(\u001b[38;5;28mself\u001b[39m, obj: HasTraits | \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28mcls\u001b[39m: \u001b[38;5;28mtype\u001b[39m[t.Any]) -> Self | G:\n\u001b[32m    677\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Get the value of the trait by self.name for the instance.\u001b[39;00m\n\u001b[32m    678\u001b[39m \n\u001b[32m    679\u001b[39m \u001b[33;03m    Default values are instantiated when :meth:`HasTraits.__new__`\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    682\u001b[39m \u001b[33;03m    is in the :class:`HasTraits` instance.\u001b[39;00m\n\u001b[32m    683\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m    684\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "236382cc",
   "metadata": {},
   "source": [
    "6. Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486b85ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"475\"\n",
       "            src=\"http://WandilePhinzi:40000/dtale/iframe/4\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x28055f53820>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtale.show(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bffe37a",
   "metadata": {},
   "source": [
    "7. Train the Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7036d7c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(n_estimators=100, max_depth=3, random_state=42)\n",
    "rf.fit(X_train, Y_train)\n",
    "\n",
    "Kfold = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "scores = cross_val_score(rf, X_train, Y_train, cv=Kfold, scoring='accuracy')\n",
    "\n",
    "Y_pred = rf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26559448",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.86      1.00      0.92     43525\n",
      "         1.0       0.00      0.00      0.00      7211\n",
      "\n",
      "    accuracy                           0.86     50736\n",
      "   macro avg       0.43      0.50      0.46     50736\n",
      "weighted avg       0.74      0.86      0.79     50736\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Frank\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning:\n",
      "\n",
      "Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "\n",
      "C:\\Users\\Frank\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning:\n",
      "\n",
      "Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "\n",
      "C:\\Users\\Frank\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning:\n",
      "\n",
      "Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rf.score(X_test, Y_test)\n",
    "print(classification_report(Y_test, Y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "018ddb32",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf2 = RandomForestClassifier(\n",
    "    \n",
    "    n_estimators=1000,\n",
    "    criterion= 'entropy',\n",
    "    min_samples_split= 10,\n",
    "    random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "332030e9",
   "metadata": {},
   "source": [
    "8. Retraining the model using Resampled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a0d12d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0    174809\n",
      "1.0    174809\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "smote_enn = SMOTE(sampling_strategy='minority', random_state=42)\n",
    "X_train_res, Y_train_res = smote_enn.fit_resample(X_train, Y_train)\n",
    "print(pd.Series(Y_train_res).value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4e7bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestClassifier( random_state= 42)\n",
    "model.fit(X_train_res, Y_train_res) \n",
    "Kfold = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "scores = cross_val_score(model, X_train_res, Y_train_res, cv=Kfold, scoring='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb820887",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision  recall  f1-score     support\n",
      "0.0              0.8843  0.9257    0.9045  43525.0000\n",
      "1.0              0.3749  0.2688    0.3131   7211.0000\n",
      "accuracy         0.8324  0.8324    0.8324      0.8324\n",
      "macro avg        0.6296  0.5972    0.6088  50736.0000\n",
      "weighted avg     0.8119  0.8324    0.8205  50736.0000\n",
      "ROC-AUC: 0.7622\n",
      "Sensitivity: 0.2688\n",
      "Specificity: 0.9257\n",
      "Confusion Matrix:\n",
      "[[40293  3232]\n",
      " [ 5273  1938]]\n",
      "Mean Accuracy:0.8983 (+/- 0.0015)\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "y_proba = model.predict_proba(X_test)[:, 1]\n",
    "sensitivity = recall_score(Y_test, y_pred, pos_label=1)\n",
    "roc_auc = roc_auc_score(Y_test, y_proba)\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix(Y_test, y_pred).ravel()\n",
    "specificity = tn / (tn + fp)\n",
    "\n",
    "report = classification_report(Y_test, y_pred, output_dict=True)\n",
    "df_report = pd.DataFrame(report).transpose()\n",
    "\n",
    "pd.set_option(\"display.precision\", 4)\n",
    "print(df_report)\n",
    "print(f\"ROC-AUC: {roc_auc:.4f}\")\n",
    "print(f\"Sensitivity: {sensitivity:.4f}\")\n",
    "print(f\"Specificity: {specificity:.4f}\")\n",
    "print(f\"Confusion Matrix:\\n{confusion_matrix(Y_test, y_pred)}\")\n",
    "print(f\"Mean Accuracy:{scores.mean():.4f} (+/- {scores.std():.4f})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
